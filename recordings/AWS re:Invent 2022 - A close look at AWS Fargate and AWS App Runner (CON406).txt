 Thank you for joining this late afternoon session today. My name is Archena Shakanta. I'm a principal engineer at AWS. I've been with AWS for over 11 years now, and a large part of that tenure has been with the Container Services org. So I've actually had the good fortune of working on multiple Container Services during that tenure. And in fact, I've actually rotated through all of these services that we're gonna talk about today at some point. And App Runner and Fargate are especially close to my heart because I was one of the founding engineers, part of the founding team for those services. So I want to start today by telling you a little bit of the story of these services and how the product ideas for these services kind of evolved from one service to the next. Starting from EC2, which is our original compute service, all the way to App Runner, which is kind of the newest service on the block. And then we'll pull the curtains back and look at the under the hood architecture for these services. And you'll see that it's not just the product ideas that kind of have built on top of each other, but the actual architecture itself has kind of layered on top of each other. So newer services have been built on top of foundations that we laid with predecessor services. And for the under the hood part of this talk we'll go backwards. So we'll start with App Runner, which is kind of the highest abstraction service. And then we'll see how that's built on top of Fargate and how Fargate kind of built on top of ECS. And finally, security and availability are kind of the key tenets that we apply at Amazon across all AWS services, across all Amazon services. So we'll go over how for these architectures for these particular services, how kind of security and availability played a role in influencing the design. All right, so for the product idea evolution, we're gonna use this use case of a web application. So this is your standard kind of HTTP server that listens on a socket and responds to HTTP requests, right? And generally when you think of how you would run such an application, you have a VM, you install an operating system on it, you pick a language of your choice to write the application in and then the app sits on top of it. And you're probably running multiple copies of the stack for scale and redundancy. So you put a load balancer in front of it, you put an auto scaling group around it, and you probably have some build deployment pipeline. Now, before we see how to run this on AWS, I wanna pause and talk a little bit about the shared responsibility model on AWS. And some of you might have heard about this already. But the spirit of shared responsibility is that no matter what AWS service you use, not just specific to the ones we're gonna talk about today, in most cases, the availability and the security posture of your application at the end of the day is a joint responsibility between you, the customer and us AWS. So there's gonna be parts of the stack that we own, the responsibility, and there will be parts of the stack that you will own the responsibility for. And we're gonna use this lens of shared responsibility to go through each of the services in the context of that web application. So with EC2, this is the original compute service, EC2 basically took responsibility for running the physical servers and data centers and the virtualization software that runs on these physical servers. But you as the customer, you still own the VMs, what we call EC2 instances, you still own all the software that runs inside these instances. And you own hooking up the load balancer, the autoscaling group and the build deployment pipeline around these instances. Now you can use AWS managed services like application load balancer, et cetera, but kind of tying it all together and making sure that it's configured properly is still your responsibility at the end of the day. And a lot of our EC2 customers came back and told us, especially the ones that weren't, you know, core infrastructure admin personas, they said that this is still a lot of things that you as the customer have to tie it together. It's a lot of things that you have to get right. And it's a lot of services that you have to learn to run just a super simple web application. So in 2011, we launched Elastic Beanstalk. And what Beanstalk did is it said, okay, you don't have to go to each of these individual services and learn how to kind of stitch it all together. We will do that for you as a central kind of orchestration plane, which is Beanstalk. So you can just go to Elastic Beanstalk, you can describe your application and the environment in those terms. And Beanstalk will basically create a cloud formation template behind the scenes and, you know, deploy and provision all of these resources behind the scenes in your account. So the responsibility line here still doesn't shift. Because these resources still end up running in your account at the end of the day. So you have full access to these resources. You can go in and customize various aspects of these resources if you want to change things. So in that sense, you as the customer still own the responsibility for these components. And then around 2013, 2014, on EC2, this was before AWS had any container services available. On EC2, we started seeing that containers were starting to become popular. And a lot of our customers for this web application type use cases, they were actually using container technology. And what does that mean? They would, you know, install a container runtime, like Docker or container D, et cetera. And they would basically decouple the app packaging from the OS. So instead of building a monolithic AMI, they decoupled the app, you know, layered it with the language runtime and they would build a container image and deploy it as containers on these instances. And because containers provide some amount of resource isolation, you can actually co-locate multiple apps, multiple copies of the same app or even multiple, you know, different apps within the same instance. So you get, you know, all the wonderful benefits of containers, which is, you know, fast startup times, better fleet utilization, et cetera, et cetera. Now, this looks fine if you're looking at one instance, but if you're looking at, you know, hundreds of instances and thousands of applications like many customers were doing, the actual orchestration and placement logic becomes a fairly complex software problem. So when, you know, workload request comes in, how do you find the right spot on the right instance to actually go launch this application? And so we saw a lot of these, and container orchestrator projects start to crop up. MESOS was a popular one, Kubernetes is a big one today. And a lot of these orchestrators were basically large open source projects. And what customers were doing is that, you know, they would install a orchestrator specific agent on this instance, and then the agent typically talks to a control plane, which is much larger beefier piece of software that has all the smarts for running the placement logic and, you know, the execution of the container orchestration. So these container orchestrators, they're not easy pieces of software, right? These are large services that you have to run, and customers were running these control planes themselves. So they would actually launch more instances and, you know, run these open source projects themselves. And that was kind of the first opportunity we spotted, and we just felt wrong, like customers have to run more instances to manage their existing instances. So with ECS, which was released in 2015, we basically moved the container orchestration control plane bit down under the boundary to into the AWS side of the responsibility. So on the EC2 instance, you install an ECS agent and your instance is basically registered with our service, and then you can just speak our service APIs to launch containers on your instances. Now, you know, this is, oh, and then you still own, of course, the load balance thing and the autoscaling CI CD, because the instances are all still running in your account. You still kind of have to hook up all of these things together. And actually, some of the problems got even more complicated with containers, because now, autoscaling, for example, you don't just, you're not just autoscaling your instances because you've decoupled the container from the instance, you have to autoscale your instance fleet, and then you have to autoscale the containers on top of that, a similar build and for build and deployment pipeline, there's software that goes on the instance, then you need a pipeline for their software that goes in the container, et cetera. So there was still a lot of stuff that was in the customer side of this responsibility line, not to mention just the OS patching, the runtime patching, agent patching, all of that non-trivial amount of work for someone who just wants to run a web app. So in 2017, what we did is we moved this line even higher with Fargate, and Fargate was our serverless containers offering, and what that means is that we said, you as the customer, if you wanna run a container, you don't have to ever launch an EC2 instance. So we took responsibility of the underlying instance, we took responsibility of all that base layer software that's running on the instance, we run a Fargate agent, it's a slightly modified version of the open source CCS agent that we make available, but we're gonna call it the Fargate agent. So you as the customer, you can really only speak in the currency of containers, and you don't have to worry about the instance layer at all. There is still a certain aspect of load balancing, autoscaling CI-CD that you don't have to do at the instance level, but at the containers level, you still have to kind of hook everything up together. So with AppRunner, which, like I said, one of our newer services, what we did, was we said, let's focus on this use case of web applications, and see for that specific vertical, how can we make things even easier for you, the customer? So we moved that responsibility line even higher, and we said, you don't have to run the containers even. So we'll talk about what this experience looks like, but basically you don't have to run the containers in your account, you don't have to run the load balancer, you don't have to worry about the autoscaling groups, you don't have to worry about deployment pipelines. Really, all you are responsible for is your application image and all the software that goes in the application image. So what does this experience look like with AppRunner? So your teams can either start with source code directly in GitHub, or you can start with prebuilt container images in ECR, but basically you have to give us permission to access your artifacts. So if it's source code in GitHub, you have to create a connection object, but if it's image in ECR, you have to create an IM role, that gives us permissions. And then we will pull it down, and you just have to make one API call, the Create Service API call, and you get a URL in return against which your clients can start making HTTP requests. And like I said, you won't see the instances, you won't see the Fargate tasks or the containers, you won't see the load balancer, you won't see the autoscaling group. You just see this endpoint against which you can make requests, and everything magically scales as you start to send more requests to that endpoint. So what's going on under the hood of all this magic? So it's not magic. We have a VPC that we run behind the scenes. We're going to call it the AppRunner Service VPC. And if you're starting with source code, we basically own the we have managed language run times that we make available to you, so you don't even have to worry about the language layer of your application. We will layer it on to the runtime that we provide, and then we'll pump it through a build process, and then we'll generate a container image for your app. Of course, if you're starting with a container image, we just copy it over into our account. And then we basically deploy these as Fargate tasks in our AppRunner own service account. Now these Fargate tasks have to have some networking, so they live in our service VPC. They have their primary E&I as attached to the AppRunner service VPC. But they also have a secondary network interface that is attached to your VPC. So the application that you bring to us, if it needs to talk to a private database or something in your VPC, it uses the secondary network in our face to do that. So what happens when you actually send a request? So when your clients send a request to that URL that I talked about, the URL basically gets resolved. We use Route 53 behind the scenes to an NLB network load balancer that we run in our account. The NLB basically forwards it to an L7 request router, and the L7 request router will then forward it to the Fargate tasks that we've spun up for your service through that primary E&I that we talked about. So that's kind of the picture of what's going on at the AppRunner level. So what's actually going on behind the scenes of these Fargate tasks that we're launching? So now we're going into the Fargate team's responsibility. Before I talk about Fargate, I want to introduce this technology that we use called Firecracker. Firecracker is a open source virtualization software project by Amazon. And it's basically a hypervisor that's purpose built specifically for containers and functions. And Fargate uses it, and Lambda uses it. So what's actually going on here? So you can take any bare metal server. And then instead of the hypervisor, you basically install Firecracker. And Firecracker will spin up what we call micro VMs. Now, these are special. They're not traditional VMs. They're different from traditional VMs. That's why they're called micro VMs. Because what we've done is we've basically ripped out some of the kind of devices and things from a traditional VM in order to optimize for the startup time. So these are things that add latency to VM bootstrap. And by doing that, and these are devices that, especially for abstracted workloads like containers and functions in the cloud, a lot of these devices aren't used. So we could kind of safely pair that down to gain advantages on startup times. So with these Firecracker micro VMs, we were able to go from traditional VM bootstrap time as probably tens of seconds, if not minutes, to basically subsecond bootstrap time. So we can launch these micro VMs basically just in time as those requests are coming in to launch containers. And so that's how they're different from traditional VMs. But how are they similar to traditional VMs? So the thing about these micro VMs is that the boundary between two micro VMs that are running on the same bare metal server, it's still using the same traditional kind of VM level isolation. So we can safely co-locate multiple VMs on the same bare metal server. And you basically get EC2 instance level isolation between workloads that are running on the same server. And as you can see in this picture, each micro VM has its own guest kernel. So if we put multiple Fargate tasks, they're not sharing the guest OS or the guest kernel at all. So how has this been applied to Fargate? So the Fargate team runs their own VPC. We're going to call that the Fargate service VPC. And within the Fargate VPC, they run these bare metal instances. These are EC2 bare metal instances. They're publicly available. All of you can run it. It's basically you get the whole machine. So you can actually install virtualization software on these machines, unlike traditional other types of EC2 instances. And because these bare metal instances are running in the Fargate VPC, it has an E&I elastic network interface that's in the Fargate service VPC. So within this instance, like I said, we install the Firecracker or VMM. And we install container D. And we install this piece of kind of glue code between Firecracker and container D, which is basically when we call container D to launch the container, rather than using traditional C groups and namespaces to spin up the container, it will actually turn around and speak the Firecracker APIs to spin up the container within a micro VM. So that's what that glue code is doing. And then we run our Fargate agent. And then we basically have these micro VMs that are running your actual application container within it. And then we have E&Is at the micro VM layer, separate from the E&I for the actual bare metal instance. There's one, there's their dual-homed. So like we talked about before, there's one E&I attached to this micro VM that talks to the app runner VPC. And then there's a secondary E&I attached to the VM that talks to your VPC. So that's kind of the Fargate story. So moving on to the ECS orchestration part of this story. So ECS, like we talked about, it's an orchestrator. And its job is really to, when we make a request of the ECS control claim to launch a Fargate task, its job is to find an appropriate slot on an appropriate bare metal instance and speak to the Fargate agent to actually make the launch happen. So the control plane itself is actually pretty sophisticated mesh of multiple microservices. And I'm not going to go into a lot of details into each service. But those of you who've used ECS, you might recognize some of the things there, like task definitions, clusters, services. These are all concepts in ECS. And we basically have a microservice that owns each concept in ECS. But the way this works is that when we launch all these bare metal instances in the Fargate VPC and the Fargate agent bootstraps, the Fargate agent basically speaks out through that primary bare metal instance E&I and registers the bare metal instance with one of the microservices, which we're going to call the agent communication service. So that's the service that owns all these incoming connections from the actual worker nodes. And then when AppRunner calls ECS, AppRunner is a client of ECS, calls ECS to launch that Fargate task in the AppRunner VPC. Basically, the placement algorithm in ECS lights up. And it picks a specific bare metal instance. And the agent communication service is called upon to actually communicate with the Fargate agent and send it the command and all the configuration that the agent needs. The agent then speaks to Firecracker ContainerD or it speaks ContainerD. And the micro VM is spun up with your application container in it. So how do security considerations layer on top of this? So let's start with AppRunner. So far, we looked at the case of a single service and how it works, a single AppRunner service, and how the logistics work. But in reality, these VPCs that we're running, they're hugely multi-tenant VPCs. So we actually put, I mean, I'm showing two here, but we actually put many, many different AppRunner services into this VPC. And we have some pretty strict controls that we have to put in place to make sure that there's no, undesired breakages from one to the other, from one tenant to the other. So we use actually a lot of the controls that are available to all of you as customers of VPC. So the first one is we basically use security groups for these tasks that don't allow any task to task communication, not even within tasks of the same service or the same tenant. There's no reason why these tasks should be talking to each other at all, so we completely block that communication. The only communication that should be coming into these tasks is the requests that are flowing in from the load balancer and the request router. But as I mentioned, shared responsibility. The secondary E&I that these target tasks are attached to, that kind of segues into your, the customer's VPC, the security groups on that is the customer's responsibility to configure what kind of outbound traffic you want to allow from your application that's running in Apprunner. And this is kind of a recently launched feature, so this is on the inbound traffic side of things. We just announced private service endpoints for Apprunner, and what this means is that instead of getting an Apprunner service URL that's publicly reachable over the internet, you can now create a private end point in your VPC to accept the incoming traffic, and what we're doing is really we're creating a private link end point in your VPC, and again, the security groups that you configure on that sort of inbound connection in your VPC would be your responsibility to configure exactly which clients within the VPC can talk to your service. All right, fargate data plane security. So, zooming into this fargate VPC, like we said, we have these bare metal instances, there obviously isn't just one bare metal instance, we have many, many bare metal instances that we run per VPC, so a similar kind of theme, we don't allow any bare metal instance to instance communication through that primary E&I, no reason why instances should be talking to each other. The only communication that's allowed is the communication that the fargate agent needs to perform to the ECS control plane, and just any other kind of image pull and things like that, that the agent needs to perform, so very controlled, the security groups there. We do, like I mentioned, one of the benefits of Firecracker is that it allows us to safely place multiple tenants on the same machine, so we do actually place multiple tenants, tasks for multiple tenants, and these are not just app runner workloads, these are any public fargate workloads, we can safely co-locate them on the same bare metal instance, but we will only run one task per micro VM, we will never put two tasks, even if it's from the same customer, same account, we will never put more than one fargate task in the same micro VM, at Amazon we don't trust the container boundary to be safe enough for multi-tenant isolation, which is the reason why we've kind of made this choice of just really maintaining the security posture of your task and not putting multiple tasks within the same VM boundary. There's independent network interfaces for each of those micro VM, so your tasks are not kind of cross communicating through the same network interface, they have their dedicated E&Is that are connected to the customer VPC. And as we said, micro VM boundary hardware isolated, EC2 instance like isolation basically, and as you can see here, completely separate guest OS's and guest kernels between the multi-tenant workloads. So all of these things basically ensures that we don't have any side words breakages from tenant to tenant or that there's no downward breakages to kind of some of the fargate layer software running on the instance. All right, so going into the ECS control plane security, so ECS, the kind of protected resource, is all the state that ECS is keeping about your, about the bare metal instances, about the fargate tests that are running on the bare metal instances, and a lot of kind of the security aspect of ECS is making sure that that state is accessed and modified in a properly authenticated way. And there's basically two entry points to the control plane. Many of these services are internal services, but there's two entry points from the outside internet. And the first one is this agent communication service. So we talked about these, with fargate these bare metal instances and the agent talking to the agent communication service. And this is fairly safe because it's our agent, it's running on our instances, so we're all friends and we don't generally expect any malicious activity between fargate and ECS. But remember that the ECS control plane is not just serving fargate, right? It's also serving the EC2 launch type, which is customers running their own instances as worker nodes and having these instances register with the ECS control plane. So the agent communication service also fields these connections from all of these customer-owned instances as well. And that's the reason why we, because the agent is just, it's a piece of software for this EC2 launch type, it's a piece of software that's running out there in the wild on a customer instance, it's just an open source piece of software really, they can write their own custom version of the agent and try and talk to our APIs. So we actually don't consider the agent to be within the trust boundary. And the way we kind of enforce checks and balances here is that the instance role that's present on your instance, that's the identity that the agent is going to use to talk to our control plane. And the agent communication service basically ensures that you're only modifying or reading state about other tasks or instances in your own account and you can't read across accounts. The other entry point is the front end service, of course. So this is the one that feels all the other kind of incoming APIs, task management, run task APIs and task definition APIs, cluster API service APIs. And here, there's standard, again, IAM auth, we enforce IAM auth based on the calling actor. And then limits and throttles are another important piece of really just kind of protecting fairness on the service because unlike some of those open source orchestrator projects that we talked about, those basically are run in a single tenant mode, right? One customer installs their installation of Kubernetes or Mesos and it only serves that customer. ECS is basically a multi-tenant AWS service. So we, a lot of times these limits and throttles are frustrating to customers because you have to keep coming to our account and getting it raised for your use case, but it's really to protect you from each other really. So we have to make sure that all the resources behind the scenes are fairly used across all of our customers. So that's really the spirit of limits and throttles that are basically enforced kind of right at the front door at this front end service. All right, moving on to availability. So now we're gonna start from ECS and go backwards. So ECS control line availability. We talked about this kind of web of microservices and we basically run, we don't just run one copy of that entire stack, we run a copy of that stack for every region. So there's kind of complete independence between regions. There's no service in one region that's trying to talk to a service in another region. This is all in the spirit of, you know, if a region is having an outage, we don't want to have that impact spillover to any other regions. And I think AWS has 30 something regions give or take. So we're actually running like 30 copies of the stack. And it's not just infrastructure failures, right? Even software deployments that we perform to these services, we, you know, phase them out region by region. So any kind of software related errors that we might roll out are also kind of very controlled to make sure that they don't hit multiple regions at the same time. So within a region, we actually don't just run one copy of the stack per region because we want to do better, right? We don't win, when a region is having a problem, it's not okay for us to take down the entire region and everyone that's using that region. So we actually have this notion of a cellular architecture and we actually run multiple copies of this stack within a region. And what we do, this is completely transparent to you because the customer is we basically allocate, you know, it's, I wouldn't say it's arbitrary, but there's an algorithm that basically when you, you know, create your cluster or your tasks in ECS, you get allocated to one of our cells and then all of your APIs are then routed to that particular cell via this thin kind of cell router layer. And like I said, completely transparent to you. You don't get to pick your cells, you don't see the cells or anything like that. It's really just a knob for us, again, like I said, to reduce that impact of blast radius to be sub-region. And if you look at a particular microservice, you know, within this cellular control plane, each service is actually spread across AZs. So this is again, like standard best practice that we recommend for customers. We follow the same. And really the idea there is if a single AZ is having an infrastructure failure, we're basically losing just a third of each service. And our services are scaled up enough in the other two AZs that we're able to kind of fail over the traffic from that AZ to the other two AZs. And we're able to operate with basically no customer facing impact if they're single AZ failures. So let's talk about the Fargate data plane availability. So again, similar kind of concept. That Fargate VPC that we talked about that has the bare metal instances, we're not just running one of that per region. We actually have Zonal VPCs. So we have single subnet VPCs completely separated out zone by zone. And like I said, it's not just about the Zonal failures, but we also think about what if an operator is going into VPC and then trying to perform some operations and they fat finger something. We don't want to affect more than one AZ at a time. Or software deployments, like I mentioned, they're rolled out zone by zone to make sure that if there's any problems there, we're not affecting multiple zones at the same time. And again, the whole zone being affected is not acceptable to us. So we actually run multiple single subnet VPCs per AZ. And it's blast radius protection, but it's also kind of our scaling mechanism. So the idea here is that each VPC, we've kind of tried and tested how much load a single VPC can take and it's a fixed size. So once we start to get close to capacity for one of these fixed size cells, we basically can just keep scaling out horizontally by adding more of these VPCs per AZ. The AppRunner data plane availability. So with AppRunner, it's the similar thread. It's cellular architecture with each component within the cell being striped across multiple AZs. So the AppRunner service VPC for a given region, we run multiple VPCs per cell. Similar to kind of the ECS control plane picture, your AppRunner service gets allocated to one of the cells arbitrarily transparent to you. And if you look at kind of a single VPC, within the VPC, the Fargate task, basically every component, right? We have the load balancer, the L7 request router, the Fargate task, they're all striped across AZs. So that's kind of bringing me to the end of my talk. And really, the points I wanted to make here is we have a pretty rich portfolio for hosting your container applications. And often, it can be confusing to decide which service you should use for your application. But it's important to understand kind of the different abstraction which each service is providing for you. And my rule for these questions that we get is to always start with the highest abstraction service and then start your experiments there, really, only move down lower to the stack if you find specific reasons why the higher abstraction services don't work for you. And of course, come back and tell us. So we can tell you if that's something that's coming up, that we want to support, or if it's a feature that we're like, no, that's just never going to be built into that abstraction layer. So the lower abstraction layer is the right layer for you. And as you can see, a lot of thought has been put in to kind of the security and availability. The stance of these services, and especially for the higher abstraction services, like it has all of that thought put into every single layer beneath it. And the lower down you go, like you basically start owning the security and availability stance of those kind of layers beneath you. And it's hard to put a dollar amount on that. So really take advantage, I guess, of the value proposition of all the work that we do behind the scenes, and all the work that our teams do behind the scenes. And like I said, try to use the higher abstraction services as far as possible. That's all I had.